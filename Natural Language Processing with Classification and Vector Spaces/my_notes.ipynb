{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd088b4a69311a6c5d2f36677fc8f93d7937fcfbc5d562502f49b4c29e65bd4a61a",
   "display_name": "Python 3.7.7 64-bit ('tf_2x': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "88b4a69311a6c5d2f36677fc8f93d7937fcfbc5d562502f49b4c29e65bd4a61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3 NLP packages\n",
    "* NLTK\n",
    "* spaCy\n",
    "* gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "import numpy as np \n",
    "import random                              # pseudo-random number generator\n",
    "import matplotlib.pyplot as plt            # library for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.4.5\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/nejada/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    " # twitter data\n",
    " nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nejada/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# download the stopwords from NLTK\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "source": [
    "## Processing Tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I haven't seen that many 'menacing' since I finished JoJo :D\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['seen', 'mani', 'menac', 'sinc', 'finish', 'jojo', ':D']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tweet=random.choice(all_positive_tweets)\n",
    "print(tweet)\n",
    "process_tweet(tweet)"
   ]
  },
  {
   "source": [
    "***\n",
    "# Stemming Overview:\n",
    "\n",
    "Stemming is a method of normalization of words in Natural Language Processing. It is a technique in which a set of words in a sentence are converted into a sequence to shorten its lookup. In this method, the words having the same meaning but have some variations according to the context or sentence are normalized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stemming Algorithms\n",
    "* Rule based (Truncating)\n",
    "    1. Lovins\n",
    "    2. <u>Porter</u>\n",
    "    3. Paice/Husk\n",
    "    4. Dawson\n",
    "    5. <u>Snowball</u>\n",
    "    6. Lancaster\n",
    "* Statistical\n",
    "    1. N-Gram\n",
    "    2. HMM\n",
    "    3. YASS\n",
    "* Mixed\n",
    "    1. Krovetz\n",
    "    2. Xerox\n",
    "    3. Corpus based\n",
    "    4. Context sensitive "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Two basic stemming types\n",
    "\n",
    "* <i>Dictionary-based</i>: uses lists of related words\n",
    "* <i>Algorithmic</i>: uses program to determine related words\n",
    "\n",
    "*Algorithmic stemmers:\n",
    "\n",
    "    * suffix-s: remove ‘s’ endings assuming plural\n",
    "         * e.g., cats ! cat, lakes ! lake, wiis ! wii\n",
    "         * Many false positives: supplies ! supplie, ups ! up\n",
    "         * Some false negatives: mice \" mice (should be mouse)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ```NLTK``` Stemming packages:\n",
    "Following packages are available in ```nltk```:\n",
    "* ```arlstem```: light Arabic stemmer\n",
    "* ```cistem```: Stemmer for German\n",
    "* ```isri```: Arabic stemmer based on algorithm: Arabic Stemming without a root dictionary\n",
    "* ``` lancaster```: A word stemmer based on the Lancaster stemming algorithm\n",
    "* ```porter```: Porter stemming algorithm\n",
    "* ```regexp```: A stemmer that uses regular expressions to identify morphological affixes. Any substrings that match the regular expressions will be removed\n",
    "* ```rslp```: A stemmer for Portuguese\n",
    "* ```snowball```: This module provides a port of the Snowball stemmers developed by Martin Porter.\n",
    "* ```wordnet```: <b>Lemmatize</b> using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "\n",
    "## ```GenSim``` Stemming packages:\n",
    "* ```porter```: Porter stemming \n",
    "\n",
    "\n",
    "## ```spaCy``` Stemming packages:\n",
    "SpaCy doesn’t have a stemming library as they prefer lemmatization over stemmer while NLTK has both stemmer and lemmatizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Porter Stemmer Algorithm\n",
    "* Algorithmic stemmer used in the information retrieval (IR) experiments since the 70s\n",
    "* Consists of a series of rules designed to strip off the longest possible suffix at each step\n",
    "* Produces stems not words\n",
    "* Makes a number of errors and difficult to modify "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['compute', 'computer', 'computed', 'computing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compute --> comput\ncomputer --> comput\ncomputed --> comput\ncomputing --> comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "source": [
    "## Snowball Stemmer Algorithm\n",
    "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it.\n",
    "It was originally designed and built by Martin Porter therefore it is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compute --> comput\ncomputer --> comput\ncomputed --> comput\ncomputing --> comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow=SnowballStemmer(language='english')\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + snow.stem(token))"
   ]
  },
  {
   "source": [
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building Frequencies "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "freqs[('twice', 0.0)]"
   ]
  },
  {
   "source": [
    "### Train/Test Split "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}